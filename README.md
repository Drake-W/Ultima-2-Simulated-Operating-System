# Ultima 2.0 
This report details the design and development of the Ultima 2.0 project, a simulated operating system for C435. This program is written in C++ and is designed to run on a Unix operating system. This program and design are meant to be added to and upgraded in future iterations to add more features. This report includes the full source code as well as design diagrams. The first phase is to implement resource synchronization for our simulated operating system. This needs to be used to control access to any shared resources such as printers, memory, or disks. This synchronization is done with the use of semaphores and it is imperative to support the use of any number of semaphores for future us and iterations of this program. A process table is also needed to dynamically store Task Control Blocks (TCBs). TCBs are used to preserve the state of each task, running, ready, blocked, or dead. The TCB will be used to store more information in the future also. Output must show that the scheduler, process table, and semaphore work properly. To do this dump functions for the process table and semaphore must be added. The text book Modern Operating Systems by Tanenbaum, C435 class notes, Unix manual, and some online research was used for external information.

Full report in the documentation folder. 

Phase 1
This phase is to implement resource synchronization for our simulated operating system. This needs to be used to control access to any shared resources such as printers, memory, or disks. This synchronization is done with the use of semaphores and it is imperative to support the use of any number of semaphores for future us and iterations of this program. A process table is also needed to dynamically store Task Control Blocks (TCBs). TCBs are used to preserve the state of each task, running, ready, blocked, or dead. The TCB will be used to store more information in the future also. Output must show that the scheduler, process table, and semaphore work properly. To do this dump functions for the process table and semaphore must be added. 

Phase 2
Before this phase could be started, we had to rework Phase One. The first thing we did was assign the UI loop to its own thread. The second thing was set each thread status to “READY” upon creation. We developed a run() function in the scheduler class to set the first thread to “RUNNING”. Our yield() method was developed to set the current “RUNNING” thread to “READY”, and the find the next “READY” thread and set it to “RUNNING”. Lastly, we reworked our up() and down() methods in the semaphore class to operate as binary semaphores. If the semaphore was already being used, the thread requesting access will get queued until the current thread is finished with the resource for that semaphore.

This phase is to develop and implement a simulated interprocess communication system for the threads in our simulated operating system. One task must be able to send and receive messages from another task. This is done by the use of a physical mailbox queue in the Task Control Block (TCB). The IPC class includes a struct object that includes data members such as the Task ID numbers of the source and destination threads, message arrival time, message size, and the text of the message itself. Included class methods are Message_Send() and Message_Receive() for sending and receiving, Message_Count(int Task_Id) to count the number of messages in a specific task’s mailbox and Message_Count() to tally the number of messages in all collective mailbox queues, Message_Print() to print all queued messages for any given Task ID, Message_DeleteAll() to delete all queued messages in a given task, and ipc_Message_Dump() to print all queued messages for all tasks.
•	Each mailbox is given its own semaphore to keep threads from simultaneously writing to the same message queue.
•	Message_Send() takes an object of type Message* and enqueues it to the mailbox queue of that message’s destination Task ID.
•	Message_Receive() takes in a Task ID and dequeues the first message object on that task’s mailbox queue.
•	Message_Print() takes in a Task ID and prints all messages for that task as well as the other Message object data members.
•	Message_DeleteAll() takes a Task ID and deletes (dequeues) all messages in that task’s mailbox.
•	ipc_Message_Dump() prints all queued messages for all tasks to the Message Dump Window by printing a table header for the data members and then calling Message_Print() once for each Task that has not yet been killed

Phase 3
This phase is to develop and implement a simulated a memory management system for the threads in our simulated operating system. One task must be able to write and read to memory according to a first-fit algorithm. Memory is a linked list of eight MemNode structs of 128 “bits” that add up to a total memory cap of 1024 “bits”. The mem_mgr class contains the MemNode struct, which includes type int data members base, limit, handle, linked and current_location, as well as a string type owner, two bool type status and start, and then a MemNode pointer next. Class mem_mgr methods include a constructor, a create_node(), MemAlloc(), Mem_Free(), Mem_Read() (including an overloaded version), Mem_Write() (including an overloaded version), First_Fit(), Mem_Left(), Mem_Smallest(), Mem_Coalesce(), Mem_Dump(), Core_Dump(), Mem_Usage(). Lastly, an array of char called Mem_Core[] is included to contain the actual contents of the memory space, whether written to, empty, or freed.  
•	Each time a thread is set to RUN, it writes a single random character to the first available memory block it finds, and that section of memory is “owned” by that thread so that other threads cannot write to it. 
•	Before the scheduler tells a thread to yield, it then reads the first character in the memory block that it owns.
•	Once the character is read, it can be overwritten by the next Mem_write() call as long as that call is performed by the owning thread.
•	Once a thread is killed, its memory is “freed” by replacing any characters within the block with # signs.
•	Other methods are included to find the size of the smallest and largest blocks of free memory, as well as how much total memory is free in the whole system.
